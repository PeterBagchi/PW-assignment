{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKoZflo9MWNRmTSC0R8RVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterBagchi/PW-assignment/blob/main/Ensemble_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8e0584"
      },
      "source": [
        "## Theoretical\n",
        "\n",
        "1.  **Can we use Bagging for regression problems?**\n",
        "    Yes, Bagging can absolutely be used for regression problems. It's known as a Bagging Regressor. Instead of taking a majority vote (as in classification), it averages the predictions of individual regressors to produce a final prediction.\n",
        "\n",
        "2.  **What is the difference between multiple model training and single model training?**\n",
        "    *   **Single Model Training:** Involves training one model (e.g., a single Decision Tree, a single Neural Network) on the entire dataset. Its performance depends solely on that one model.\n",
        "    *   **Multiple Model Training (Ensemble Learning):** Involves training multiple base models, often of the same type, on variations of the training data or using different learning algorithms. The predictions from these multiple models are then combined (e.g., by voting or averaging) to produce a final, more robust prediction. The goal is to leverage the wisdom of crowds.\n",
        "\n",
        "3.  **Explain the concept of feature randomness in Random Forest.**\n",
        "    Feature randomness in Random Forest means that for each individual decision tree in the forest, when it's deciding how to split a node, it doesn't consider all available features. Instead, it randomly selects a subset of features to choose from. This helps to decorrelate the trees, preventing them from all making similar errors and reducing variance.\n",
        "\n",
        "4.  **What is OOB (Out-of-Bag) Score?**\n",
        "    The Out-of-Bag (OOB) score is a way to estimate the generalization error of a Bagging-based model (like Random Forest) without the need for a separate validation set. During bootstrap sampling, approximately one-third of the data is left out of each bootstrap sample. These are the \"out-of-bag\" samples. For each tree, its OOB samples can be used as a test set. The OOB score is the average prediction error on these OOB samples across all trees.\n",
        "\n",
        "5.  **How can you measure the importance of features in a Random Forest model?**\n",
        "    Feature importance in Random Forest can be measured in a couple of ways:\n",
        "    *   **Mean Decrease in Impurity (MDI) / Gini Importance:** This is the most common method. It measures how much each feature contributes to reducing impurity (e.g., Gini impurity for classification, variance for regression) across all splits in all trees in the forest. Features that lead to more significant impurity reductions are considered more important.\n",
        "    *   **Permutation Importance:** This method is more robust. It works by permuting the values of a single feature in the validation set and observing how much the model's performance decreases. A large decrease indicates that the feature is important.\n",
        "\n",
        "6.  **Explain the working principle of a Bagging Classifier.**\n",
        "    A Bagging Classifier works by:\n",
        "    1.  **Bootstrap Aggregating (Bagging):** It creates multiple subsets of the original training data by randomly sampling with replacement (bootstrap samples).\n",
        "    2.  **Base Model Training:** It trains an independent base classifier (e.g., a Decision Tree) on each of these bootstrap samples.\n",
        "    3.  **Aggregation:** For a new, unseen data point, each trained base classifier makes a prediction. The Bagging Classifier then combines these predictions using a majority vote (for classification) to determine the final class label.\n",
        "\n",
        "7.  **How do you evaluate a Bagging Classifier's performance?**\n",
        "    A Bagging Classifier's performance can be evaluated using standard classification metrics, typically on a separate test set:\n",
        "    *   **Accuracy:** For overall correctness.\n",
        "    *   **Precision, Recall, F1-score:** For class-specific performance, especially in imbalanced datasets.\n",
        "    *   **ROC AUC (Receiver Operating Characteristic Area Under the Curve):** For models that output probabilities.\n",
        "    *   **Confusion Matrix:** To understand the types of errors made.\n",
        "    *   **Out-of-Bag (OOB) Score:** As mentioned in question 4, this provides an internal, unbiased estimate of generalization error during training.\n",
        "\n",
        "8.  **How does a Bagging Regressor work?**\n",
        "    A Bagging Regressor works similarly to a Bagging Classifier, but with an adjustment for regression tasks:\n",
        "    1.  **Bootstrap Aggregating:** It creates multiple bootstrap samples from the original training data.\n",
        "    2.  **Base Model Training:** It trains an independent base regressor (e.g., a Decision Tree Regressor) on each bootstrap sample.\n",
        "    3.  **Aggregation:** For a new data point, each trained base regressor predicts a numerical value. The Bagging Regressor then combines these predictions by averaging them to produce the final predicted output.\n",
        "\n",
        "9.  **What is the main advantage of ensemble techniques?**\n",
        "    The main advantage of ensemble techniques is that they typically lead to improved predictive performance (higher accuracy/lower error) and increased robustness compared to single models. They achieve this by reducing variance (like Bagging) or bias (like Boosting), making them less prone to overfitting and more stable.\n",
        "\n",
        "10. **What is the main challenge of ensemble methods?**\n",
        "    The main challenges of ensemble methods include:\n",
        "    *   **Increased Computational Cost:** Training and predicting with multiple models require more computational resources and time than a single model.\n",
        "    *   **Interpretability:** Ensembles can be harder to interpret and understand than simpler individual models, especially for complex stacking or boosting methods.\n",
        "    *   **Memory Usage:** Storing multiple models can consume more memory.\n",
        "    *   **Diminishing Returns:** After a certain number of base models, adding more models might not lead to significant performance gains, but still incurs additional cost.\n",
        "\n",
        "11. **Explain the key idea behind ensemble techniques.**\n",
        "    The key idea behind ensemble techniques is that a group of weak learners can collectively form a strong learner. By combining the predictions of multiple diverse models, the ensemble can often achieve better performance (higher accuracy, better generalization) than any single model alone. This is often attributed to reducing bias, variance, or both.\n",
        "\n",
        "12. **What is a Random Forest Classifier?**\n",
        "    A Random Forest Classifier is an ensemble learning method for classification that operates by constructing a multitude of decision trees during training. For classification tasks, it outputs the class that is the mode of the classes output by individual trees. It incorporates two main sources of randomness:\n",
        "    *   **Bootstrap Aggregating (Bagging):** Each tree is trained on a different bootstrap sample of the training data.\n",
        "    *   **Feature Randomness:** When splitting a node, each tree considers only a random subset of features, rather than all features.\n",
        "\n",
        "13. **What are the main types of ensemble techniques?**\n",
        "    The main types of ensemble techniques are:\n",
        "    *   **Bagging (Bootstrap Aggregating):** Trains multiple similar models independently on different bootstrap samples of the data, and then averages or votes their predictions (e.g., Random Forest).\n",
        "    *   **Boosting:** Trains multiple models sequentially, where each subsequent model tries to correct the errors of the previous ones (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM).\n",
        "    *   **Stacking (Stacked Generalization):** Trains multiple base models and then trains a meta-model (or \"learner\") to learn how to best combine the predictions of the base models.\n",
        "\n",
        "14. **What is ensemble learning in machine learning?**\n",
        "    Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained to solve the same problem. Unlike traditional machine learning approaches which try to build one powerful model, ensemble methods combine the predictions of several base models to obtain better predictive performance than could be obtained from any of the constituent base models alone.\n",
        "\n",
        "15. **When should we avoid using ensemble methods?**\n",
        "    We might avoid using ensemble methods when:\n",
        "    *   **Interpretability is paramount:** If understanding *why* a prediction is made is more important than the prediction itself, simpler models might be preferred due to the black-box nature of many ensembles.\n",
        "    *   **Computational resources are severely limited:** Ensembles are generally more computationally expensive and memory-intensive.\n",
        "    *   **The base models are already very highly correlated:** If all base models make similar errors, combining them might not lead to significant improvement.\n",
        "    *   **The dataset is very small:** The benefits of diversity and variance reduction might not be fully realized.\n",
        "\n",
        "16. **How does Bagging help in reducing overfitting?**\n",
        "    Bagging helps in reducing overfitting primarily by **reducing variance**. Each base model (e.g., a decision tree) in a Bagging ensemble is prone to overfitting the specific bootstrap sample it was trained on. However, by training many such models on different bootstrap samples and then averaging (for regression) or voting (for classification) their predictions, the ensemble effectively smooths out the individual models' overfitting tendencies. The errors that individual models make due to overfitting to their specific data variations tend to cancel each other out, leading to a more generalized and stable prediction.\n",
        "\n",
        "17. **Why is Random Forest better than a single Decision Tree?**\n",
        "    Random Forest is generally better than a single Decision Tree because:\n",
        "    *   **Reduced Overfitting:** A single decision tree is very prone to overfitting, especially deep ones, as they learn highly specific patterns from the training data. Random Forest, through Bagging and feature randomness, significantly reduces this overfitting tendency by averaging out the biases and variances of many individual trees.\n",
        "    *   **Higher Accuracy:** By combining the predictions of many decorrelated trees, Random Forest typically achieves higher predictive accuracy and better generalization performance than a single decision tree.\n",
        "    *   **Robustness:** It is more robust to noise and outliers in the data.\n",
        "    *   **Feature Importance:** It provides a reliable way to estimate feature importance.\n",
        "\n",
        "18. **What is the role of bootstrap sampling in Bagging?**\n",
        "    The role of bootstrap sampling in Bagging is crucial for creating diversity among the base models. By sampling the training data with replacement, each base model is trained on a slightly different subset of the original data. This ensures that the individual models are not identical, and their errors are not perfectly correlated. This diversity is essential for the ensemble to be effective in reducing variance and improving overall performance when their predictions are combined.\n",
        "\n",
        "19. **What are some real-world applications of ensemble techniques?**\n",
        "    Ensemble techniques are widely used in many real-world applications, including:\n",
        "    *   **Medical Diagnosis:** Predicting disease likelihood based on patient data.\n",
        "    *   **Fraud Detection:** Identifying fraudulent transactions in finance.\n",
        "    *   **Image Recognition and Computer Vision:** Object detection, facial recognition.\n",
        "    *   **Natural Language Processing:** Sentiment analysis, spam detection.\n",
        "    *   **Recommendation Systems:** Suggesting products, movies, or music to users.\n",
        "    *   **Credit Scoring:** Assessing creditworthiness of applicants.\n",
        "    *   **Weather Forecasting:** Combining predictions from various atmospheric models.\n",
        "\n",
        "20. **What is the difference between Bagging and Boosting?**\n",
        "    The main differences between Bagging and Boosting are:\n",
        "    *   **Parallel vs. Sequential:** Bagging (e.g., Random Forest) trains base learners in **parallel** and independently. Boosting (e.g., AdaBoost, Gradient Boosting) trains base learners **sequentially**, where each new learner corrects the errors of the previous ones.\n",
        "    *   **Bias vs. Variance:** Bagging primarily aims to **reduce variance** by averaging predictions of diverse models. Boosting primarily aims to **reduce bias** by iteratively focusing on misclassified or high-error instances.\n",
        "    *   **Weighting of Samples:** In Bagging, each base learner is trained on a bootstrap sample, and typically, all samples within that bootstrap sample are weighted equally. In Boosting, samples that were misclassified or had high errors by previous models are given higher weights, forcing subsequent models to focus on them.\n",
        "    *   **Final Prediction:** Bagging combines predictions by averaging (regression) or majority voting (classification). Boosting combines predictions by weighted averaging, where more accurate models have higher weights."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical"
      ],
      "metadata": {
        "id": "fmnskOzANagL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "ekKGYPTXBYMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS8TfOvrCaKT",
        "outputId": "8610d568-cf33-411c-f10f-cbcf23de7469"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "SbIa_AbOB3bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpWxMwNlCfDs",
        "outputId": "511d52bf-5bea-41bb-b465-1f880821e905"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2572988359842641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "aY93Yfy5Bc8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importances\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox03T1htCgop",
        "outputId": "23b96cac-a117-4674-a640-e859c0c99fcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "mean radius: 0.0487\n",
            "mean texture: 0.0136\n",
            "mean perimeter: 0.0533\n",
            "mean area: 0.0476\n",
            "mean smoothness: 0.0073\n",
            "mean compactness: 0.0139\n",
            "mean concavity: 0.0680\n",
            "mean concave points: 0.1062\n",
            "mean symmetry: 0.0038\n",
            "mean fractal dimension: 0.0039\n",
            "radius error: 0.0201\n",
            "texture error: 0.0047\n",
            "perimeter error: 0.0113\n",
            "area error: 0.0224\n",
            "smoothness error: 0.0043\n",
            "compactness error: 0.0053\n",
            "concavity error: 0.0094\n",
            "concave points error: 0.0035\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0053\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0217\n",
            "worst perimeter: 0.0671\n",
            "worst area: 0.1539\n",
            "worst smoothness: 0.0106\n",
            "worst compactness: 0.0203\n",
            "worst concavity: 0.0318\n",
            "worst concave points: 0.1447\n",
            "worst symmetry: 0.0101\n",
            "worst fractal dimension: 0.0052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "4V44HeKKBcxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "tree_pred = tree_reg.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, tree_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIQp3HNCiP6",
        "outputId": "4841bd06-f16f-4b12-9ebd-9dfeeb759554"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.495235205629094\n",
            "Random Forest MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "-0RxEQtfBmCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Random Forest with OOB enabled\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "print(\"OOB Score:\", rf_clf.oob_score_)\n"
      ],
      "metadata": {
        "id": "F6AK1aXIBcUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ac9ef4-ab00-4e95-97b9-458dd2b9a09c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9831460674157303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "OYgW4d4VCtDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, Y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging with SVM\n",
        "bag_svm = BaggingClassifier(\n",
        "    estimator=SVC(kernel='rbf', probability=True),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_svm.fit(X_train, y_train)\n",
        "pred = bag_svm.predict(X_test)\n",
        "\n",
        "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL5RpV-BDIYq",
        "outputId": "9dc12413-3604-439d-9ed8-94fd704227a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (SVM) Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "oSveGXIzCs3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Experiment with number of trees\n",
        "trees = [10, 50, 100, 200]\n",
        "print(\"Random Forest Accuracy Comparison:\\n\")\n",
        "\n",
        "for n in trees:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    print(f\"{n} Trees → Accuracy: {accuracy_score(y_test, pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7aziN02DJz2",
        "outputId": "76a3dd83-1fb0-424c-b985-4fc4d491d801"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy Comparison:\n",
            "\n",
            "10 Trees → Accuracy: 0.9561\n",
            "50 Trees → Accuracy: 0.9649\n",
            "100 Trees → Accuracy: 0.9649\n",
            "200 Trees → Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "\n"
      ],
      "metadata": {
        "id": "jUE9FFncCstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Logistic Regression\n",
        "bag_lr = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=500),\n",
        "    n_estimators=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_lr.fit(X_train, y_train)\n",
        "pred_prob = bag_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Bagging Logistic Regression AUC:\", roc_auc_score(y_test, pred_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16MQvN0hDLSZ",
        "outputId": "53d422de-1782-43d0-d126-b333d97970d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Logistic Regression AUC: 0.99737962659679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##29. Train a Random Forest Regressor and analyze feature importance scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "gvHDJKtvCshl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature importances\n",
        "importances = rf_reg.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnHJss2rDMyo",
        "outputId": "f29f4bc2-ef1b-49a6-ba8d-bc8fe79dd238"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "MedInc: 0.5249\n",
            "HouseAge: 0.0546\n",
            "AveRooms: 0.0443\n",
            "AveBedrms: 0.0296\n",
            "Population: 0.0306\n",
            "AveOccup: 0.1384\n",
            "Latitude: 0.0889\n",
            "Longitude: 0.0886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "5MBc1YIbCsMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Bagging with Decision Trees\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy :\", accuracy_score(y_test, bag_pred))\n",
        "print(\"Random Forest Accuracy     :\", accuracy_score(y_test, rf_pred))\n"
      ],
      "metadata": {
        "id": "tamVuywL7hrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b6a616-711f-43b3-d0a9-1520dcb09ad3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy : 0.9777777777777777\n",
            "Random Forest Accuracy     : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zYz-oRKGDQzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExvmwOIqDro9",
        "outputId": "d83ff6f3-06bc-4bc3-d815-e5dca467b3e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9582026257697223\n",
            "Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "AJ0UgKiuDQpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "estimators_list = [10, 30, 50, 100]\n",
        "\n",
        "print(\"Bagging Regressor Performance:\\n\")\n",
        "for n in estimators_list:\n",
        "    bag = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    pred = bag.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    print(f\"{n} Estimators → MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26kEGy5kDtEp",
        "outputId": "b8932c1a-27d2-4d04-f4e8-676f601465b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Performance:\n",
            "\n",
            "10 Estimators → MSE: 0.2824\n",
            "30 Estimators → MSE: 0.2612\n",
            "50 Estimators → MSE: 0.2573\n",
            "100 Estimators → MSE: 0.2559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##33. Train a Random Forest Classifier and analyze misclassified samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "ujNLW0p7DQfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train RF\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = [i for i in range(len(y_test)) if y_test[i] != pred[i]]\n",
        "\n",
        "print(\"Misclassified Samples (index → true, predicted):\\n\")\n",
        "for idx in misclassified_indices:\n",
        "    print(f\"{idx}: True={y_test[idx]}, Pred={pred[idx]}\")\n",
        "\n",
        "print(\"\\nTotal Misclassified:\", len(misclassified_indices))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cfR-IRVDuce",
        "outputId": "521a0a22-6614-4358-f762-67ce7e9d1254"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Samples (index → true, predicted):\n",
            "\n",
            "\n",
            "Total Misclassified: 0\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "\n"
      ],
      "metadata": {
        "id": "0D9X6isTDQX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_pred = tree.predict(X_test)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, tree_pred))\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e4u73xgDvv5",
        "outputId": "48c8b4e9-a8e8-4492-a6df-071b1ffe521b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "sH0CXbdbDQMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "pred = rf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "# Plot\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wCBt5IhODw4c",
        "outputId": "505aad8c-3d68-4ff8-be95-a971d0991a0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "RUPRxnOyD3fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base learners\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "]\n",
        "\n",
        "# Final estimator = Logistic Regression\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=500)\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIIC1IrFD2hJ",
        "outputId": "ac4b8f21-6d7d-4520-863e-b337ff108315"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##37. Train a Random Forest Classifier and print the top 5 most important features.\n",
        "\n"
      ],
      "metadata": {
        "id": "NpXmPPOfD30k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "features = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Sort by importance\n",
        "importance = rf.feature_importances_\n",
        "sorted_idx = importance.argsort()[::-1]\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{features[sorted_idx[i]]}: {importance[sorted_idx[i]]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owr4WZNwD63X",
        "outputId": "ab68abf5-4ebf-43ff-b26a-541f91c75522"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area: 0.1285\n",
            "worst concave points: 0.1283\n",
            "worst perimeter: 0.1271\n",
            "mean concave points: 0.1198\n",
            "worst radius: 0.0693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and Fl-score.\n",
        "\n"
      ],
      "metadata": {
        "id": "YZMzCpMcD4NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging using Decision Tree\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, pred))\n",
        "print(\"Recall:\", recall_score(y_test, pred))\n",
        "print(\"F1-Score:\", f1_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyXsokhMEX_J",
        "outputId": "1aee08c9-1a53-4edf-959d-a1c993e08da5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9583333333333334\n",
            "Recall: 0.971830985915493\n",
            "F1-Score: 0.965034965034965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "yJJE7R-ND4cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "depth_values = [2, 4, 6, 8, None]  # None = unlimited depth\n",
        "\n",
        "print(\"Effect of max_depth on Random Forest Accuracy:\\n\")\n",
        "for depth in depth_values:\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=150,\n",
        "        max_depth=depth,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    print(f\"max_depth={depth} → Accuracy: {accuracy_score(y_test, pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcmgV6jhEZWf",
        "outputId": "5dcce2e9-1027-49d2-a66b-ccbece173a07"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect of max_depth on Random Forest Accuracy:\n",
            "\n",
            "max_depth=2 → Accuracy: 0.9722\n",
            "max_depth=4 → Accuracy: 1.0000\n",
            "max_depth=6 → Accuracy: 1.0000\n",
            "max_depth=8 → Accuracy: 1.0000\n",
            "max_depth=None → Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance."
      ],
      "metadata": {
        "id": "sP6em5qKD7na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "bag_tree = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_tree.fit(X_train, y_train)\n",
        "pred_tree = bag_tree.predict(X_test)\n",
        "\n",
        "# Bagging with KNN\n",
        "bag_knn = BaggingRegressor(\n",
        "    estimator=KNeighborsRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_knn.fit(X_train, y_train)\n",
        "pred_knn = bag_knn.predict(X_test)\n",
        "\n",
        "print(\"DecisionTree + Bagging → MSE:\", mean_squared_error(y_test, pred_tree))\n",
        "print(\"KNN + Bagging         → MSE:\", mean_squared_error(y_test, pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKpNU74bD7Nu",
        "outputId": "a0380a19-4976-4f15-8bec-d1fb4cfb08d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree + Bagging → MSE: 0.2572988359842641\n",
            "KNN + Bagging         → MSE: 1.0762752887085227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "\n"
      ],
      "metadata": {
        "id": "4EXgz14nD8BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train RF\n",
        "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probability\n",
        "prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyjCWTUDD2YX",
        "outputId": "dadf5e2f-ce99-4079-f53a-79f07027b008"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9963969865705863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##42. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "\n"
      ],
      "metadata": {
        "id": "VYmc1bwaEcq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5-fold cross-validation\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-NUJ0aTD2Ph",
        "outputId": "4b216e6a-b3f1-4bba-a4c1-edd7eda26ebf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.94444444 0.88888889 0.97222222 0.97142857 1.        ]\n",
            "Mean Accuracy: 0.9553968253968254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##43. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "\n"
      ],
      "metadata": {
        "id": "uJWLCioxEdaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train RF\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Probabilities\n",
        "prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, prob)\n",
        "\n",
        "# Plot PR curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (Random Forest)\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "tzlGXXeHD2Fp",
        "outputId": "c06b1400-df7a-46e5-8e4f-c3dbea28e858"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUJpJREFUeJzt3XlclOXeP/DPzDDMgIKobIIkLim5FyYPLqGJoJip55T7RrnLSeWYiamIlmSnSDOV8rjlU4lbZmooUpgLpuHyZO6i4gaipSgIDDPX7w9/TI4zKODMPdL9eb9e89L7muu+5rq/zDAf7mVGIYQQICIiIpIRpb0nQERERCQ1BiAiIiKSHQYgIiIikh0GICIiIpIdBiAiIiKSHQYgIiIikh0GICIiIpIdBiAiIiKSHQYgIiIikh0GIJKt4cOHw9/fv0LrpKWlQaFQIC0tzSZzquo6deqETp06GZcvXLgAhUKBlStX2m1OT4NLly5Bq9Vi79699p7KY/n7+2P48OH2nsbfUv/+/dG3b197T4P+PwYgkszKlSuhUCiMN61Wi8aNGyMqKgo5OTn2nt5TrzRMlN6USiVq1aqF7t27Iz093d7Ts4qcnBxMnjwZAQEBcHZ2RrVq1RAYGIj33nsPt27dsvf0Km327NkICgpC+/btjW3Dhw83+XlqNBo0btwYM2fORGFhoR1n+3R5uE4P3pKTk+09PTNXr17FrFmzcOTIEbP73nnnHWzYsAFHjx6VfmJkxsHeEyD5mT17NurXr4/CwkLs2bMHS5YswbZt23Ds2DE4OztLNo+lS5fCYDBUaJ2XXnoJ9+7dg6Ojo41m9XgDBgxAREQE9Ho9Tp8+jcWLF6Nz5844ePAgWrRoYbd5PamDBw8iIiICd+/exeDBgxEYGAgA+PXXX/HBBx/g559/xo4dO+w8y4rLzc3FqlWrsGrVKrP7NBoN/vvf/wIAbt++je+++w5z5szBuXPn8NVXX0k91afWg3V6UKtWrewwm0e7evUq4uLi4O/vj9atW5vc9/zzz6NNmzb4+OOP8eWXX9pngmTEAESS6969O9q0aQMAGDFiBGrXro2EhAR89913GDBggMV18vPzUa1aNavOQ61WV3gdpVIJrVZr1XlU1AsvvIDBgwcblzt27Iju3btjyZIlWLx4sR1nVnm3bt1Cnz59oFKpcPjwYQQEBJjc//7772Pp0qVWeSxbPJce5X//93/h4OCAnj17mt3n4OBg8rMcN24c2rVrh2+++QYJCQnw8vKSbJ5Ps4frZE0FBQWS/uHVt29fxMbGYvHixahevbpkj0vmeAiM7O7ll18GAJw/fx7A/V3e1atXx7lz5xAREQEXFxcMGjQIAGAwGDB//nw0a9YMWq0WXl5eGD16NP7880+zcX/44QeEhITAxcUFrq6uePHFF/H1118b77d0DtCaNWsQGBhoXKdFixZYsGCB8f6yzgFat24dAgMD4eTkBHd3dwwePBhXrlwx6VO6XVeuXEHv3r1RvXp1eHh4YPLkydDr9ZWuX8eOHQEA586dM2m/desWJk6cCD8/P2g0GjRq1Ajz5s0z2+tlMBiwYMECtGjRAlqtFh4eHujWrRt+/fVXY58VK1bg5ZdfhqenJzQaDZo2bYolS5ZUes4P+/zzz3HlyhUkJCSYhR8A8PLywvTp043LCoUCs2bNMuv38PkrpYddd+3ahXHjxsHT0xN169bF+vXrje2W5qJQKHDs2DFj28mTJ/Haa6+hVq1a0Gq1aNOmDTZv3lyubdu0aROCgoLK9WanUCjQoUMHCCGQmZlpbL948SLGjRuHJk2awMnJCbVr18brr7+OCxcumKxfur179+5FdHQ0PDw8UK1aNfTp0we5ubkmfYUQeO+991C3bl04Ozujc+fO+P333y3OKzMzE6+//jpq1aoFZ2dn/M///A+2bt1q0qf0tbF27VrExcXB19cXLi4ueO2113D79m0UFRVh4sSJ8PT0RPXq1REZGYmioqJy1bA8Fi9ejGbNmkGj0cDHxwfjx483O2zaqVMnNG/eHBkZGXjppZfg7OyMadOmAQCKiooQGxuLRo0aQaPRwM/PD1OmTDGbY0pKCjp06AA3NzdUr14dTZo0MY6RlpaGF198EQAQGRlpPFT34DlwXbt2RX5+PlJSUqy27VQ53ANEdlf6xl27dm1jW0lJCcLDw9GhQwd89NFHxr/QRo8ejZUrVyIyMhJvvfUWzp8/j88++wyHDx/G3r17jXt1Vq5ciTfeeAPNmjVDTEwM3NzccPjwYSQnJ2PgwIEW55GSkoIBAwagS5cumDdvHgDgxIkT2Lt3LyZMmFDm/Evn8+KLLyI+Ph45OTlYsGAB9u7di8OHD8PNzc3YV6/XIzw8HEFBQfjoo4+wc+dOfPzxx2jYsCHGjh1bqfqVvgnWrFnT2FZQUICQkBBcuXIFo0ePxjPPPIN9+/YhJiYG165dw/z5841933zzTaxcuRLdu3fHiBEjUFJSgt27d2P//v3GPXVLlixBs2bN8Oqrr8LBwQHff/89xo0bB4PBgPHjx1dq3g/avHkznJyc8Nprrz3xWJaMGzcOHh4emDlzJvLz89GjRw9Ur14da9euRUhIiEnfpKQkNGvWDM2bNwcA/P7772jfvj18fX0xdepUVKtWDWvXrkXv3r2xYcMG9OnTp8zH1el0OHjwYIV+tpZ+ngcPHsS+ffvQv39/1K1bFxcuXMCSJUvQqVMnHD9+3GwPxr/+9S/UrFkTsbGxuHDhAubPn4+oqCgkJSUZ+8ycORPvvfceIiIiEBERgUOHDiEsLAzFxcUmY+Xk5KBdu3YoKCjAW2+9hdq1a2PVqlV49dVXsX79erPtj4+Ph5OTE6ZOnYqzZ89i4cKFUKvVUCqV+PPPPzFr1izs378fK1euRP369TFz5sxy1eXGjRsmy2q1GjVq1AAAzJo1C3FxcQgNDcXYsWNx6tQpLFmyBAcPHjT5vQAAN2/eRPfu3dG/f38MHjwYXl5eMBgMePXVV7Fnzx6MGjUKzz33HH777Td88sknOH36NDZt2gTg/nPhlVdeQcuWLTF79mxoNBqcPXvWeHL7c889h9mzZ2PmzJkYNWqU8Y+Tdu3aGR+/adOmcHJywt69ex/53CEJCCKJrFixQgAQO3fuFLm5ueLSpUtizZo1onbt2sLJyUlcvnxZCCHEsGHDBAAxdepUk/V3794tAIivvvrKpD05Odmk/datW8LFxUUEBQWJe/fumfQ1GAzG/w8bNkzUq1fPuDxhwgTh6uoqSkpKytyGn376SQAQP/30kxBCiOLiYuHp6SmaN29u8lhbtmwRAMTMmTNNHg+AmD17tsmYzz//vAgMDCzzMUudP39eABBxcXEiNzdXZGdni927d4sXX3xRABDr1q0z9p0zZ46oVq2aOH36tMkYU6dOFSqVSmRlZQkhhPjxxx8FAPHWW2+ZPd6DtSooKDC7Pzw8XDRo0MCkLSQkRISEhJjNecWKFY/ctpo1a4pWrVo9ss+DAIjY2Fiz9nr16olhw4YZl0ufcx06dDD7uQ4YMEB4enqatF+7dk0olUqTn1GXLl1EixYtRGFhobHNYDCIdu3aiWefffaR8zx79qwAIBYuXGh237Bhw0S1atVEbm6uyM3NFWfPnhUfffSRUCgUonnz5o+tf3p6ugAgvvzyS7PtDQ0NNVl/0qRJQqVSiVu3bgkhhLh+/bpwdHQUPXr0MOk3bdo0AcCkhhMnThQAxO7du41td+7cEfXr1xf+/v5Cr9cLIf56bTRv3lwUFxcb+w4YMEAoFArRvXt3k/kHBwebvP7KUvq6efhW+jwr3ZawsDDjXIQQ4rPPPhMAxPLly41tISEhAoBITEw0eYzVq1cLpVJpso1CCJGYmCgAiL179wohhPjkk08EAJGbm1vmfA8ePPjY53zjxo3N6kHS4yEwklxoaCg8PDzg5+eH/v37o3r16vj222/h6+tr0u/hv5rXrVuHGjVqoGvXrrhx44bxFhgYiOrVq+Onn34CcH9Pzp07dzB16lSz83UUCkWZ83Jzc6vwrulff/0V169fx7hx40weq0ePHggICDA7TAAAY8aMMVnu2LGjyeGOx4mNjYWHhwe8vb3RsWNHnDhxAh9//LHJ3pN169ahY8eOqFmzpkmtQkNDodfr8fPPPwMANmzYAIVCgdjYWLPHebBWTk5Oxv/fvn0bN27cQEhICDIzM3H79u1yz70seXl5cHFxeeJxyjJy5EioVCqTtn79+uH69esmhzPXr18Pg8GAfv36AQD++OMP/Pjjj+jbty/u3LljrOPNmzcRHh6OM2fOmB3qfNDNmzcBmO7NeVB+fj48PDzg4eGBRo0aYfLkyWjfvj2+++67Muuv0+lw8+ZNNGrUCG5ubjh06JDZuKNGjTJZv2PHjtDr9bh48SIAYOfOnSguLsa//vUvk34TJ040G2vbtm1o27YtOnToYGyrXr06Ro0ahQsXLuD48eMm/YcOHWqyxyUoKAhCCLzxxhsm/YKCgnDp0iWUlJRYrM2DtFotUlJSTG4ff/yxybZMnDgRSuVfb2kjR46Eq6ur2WtQo9EgMjLSpG3dunV47rnnEBAQYPJ6KT08X/q7pXRv7nfffVfhCygeVPq6JPviITCS3KJFi9C4cWM4ODjAy8sLTZo0MfnFBdw/6bFu3bombWfOnMHt27fh6elpcdzr168D+OuQWukhjPIaN24c1q5di+7du8PX1xdhYWHo27cvunXrVuY6pW8oTZo0MbsvICAAe/bsMWkrPcfmQTVr1jQ5hyk3N9fknKDq1aubnD8yatQovP766ygsLMSPP/6ITz/91OwcojNnzuD//u//zB6r1IO18vHxQa1atcrcRgDYu3cvYmNjkZ6ejoKCApP7bt++bTwUUVmurq64c+fOE43xKPXr1zdr69atG2rUqIGkpCR06dIFwP3DX61bt0bjxo0BAGfPnoUQAjNmzMCMGTMsjn39+nWz8P4wIYTFdq1Wi++//x4AcPnyZXz44Ye4fv26SeABgHv37iE+Ph4rVqzAlStXTMazFECfeeYZk+XSAFb6PCt93j777LMm/Tw8PMzC2sWLFxEUFGT2GM8995zx/gdfaw8/dulzw8/Pz6zdYDDg9u3bJoe/LVGpVAgNDbV4X1mvQUdHRzRo0MB4fylfX1+zqzjPnDmDEydOPPb10q9fP/z3v//FiBEjMHXqVHTp0gX/+Mc/8Nprr5n9DnsUIcQj/xgjaTAAkeTatm1rPLekLBqNxuwXisFggKenZ5mXB5f1y6u8PD09ceTIEWzfvh0//PADfvjhB6xYsQJDhw61eAlzZTy8F8KSF1980eSXdmxsrMkJv88++6zxzeCVV16BSqXC1KlT0blzZ2NdDQYDunbtiilTplh8jNI3+PI4d+4cunTpgoCAACQkJMDPzw+Ojo7Ytm0bPvnkkyf6S7hUQEAAjhw5guLi4if6iIGyTiZ/OFAA959jvXv3xrfffovFixcjJycHe/fuxdy5c419Srdt8uTJCA8Ptzh2o0aNypxP6Ru7pZP0AfM39vDwcAQEBGD06NEmJ1n/61//wooVKzBx4kQEBwejRo0aUCgU6N+/v8X6l/U8KyuIWVNZj23POT3I0nPBYDCgRYsWSEhIsLhOaXhzcnLCzz//jJ9++glbt25FcnIykpKS8PLLL2PHjh3len0D958PD4dPkh4DEFUZDRs2xM6dO9G+fXuLv8Qe7AcAx44de+SbkyWOjo7o2bMnevbsCYPBgHHjxuHzzz/HjBkzLI5Vr149AMCpU6eMu8tLnTp1ynh/RXz11Ve4d++ecblBgwaP7P/uu+9i6dKlmD59uvGD4Ro2bIi7d++W+VdzqYYNG2L79u34448/ytwL9P3336OoqAibN282+eu+9LCANfTs2RPp6enYsGFDmR+F8KCaNWuaXeFTXFyMa9euVehx+/Xrh1WrViE1NRUnTpyAEMJ4+Av4q/ZqtfqxtbTkmWeegZOTk/EKx8epU6cOJk2ahLi4OOzfvx//8z//A+D+oblhw4YZD/sAQGFhYaU/HLL0eXnmzBmT51dubq5ZWKtXrx5OnTplNsbJkydNxrKXB1+DD25LcXExzp8/X66fW8OGDXH06FF06dLlsXtmlEolunTpgi5duiAhIQFz587Fu+++i59++gmhoaGPXb+kpASXLl3Cq6++Wo6tI1viOUBUZfTt2xd6vR5z5swxu6+kpMT4ZhAWFgYXFxfEx8ebfaLuo/7aLD1fo5RSqUTLli0BoMzLddu0aQNPT08kJiaa9Pnhhx9w4sQJ9OjRo1zb9qD27dsjNDTUeHtcAHJzc8Po0aOxfft246fP9u3bF+np6di+fbtZ/1u3bhnPu/jnP/8JIQTi4uLM+pXWqvSv2ocPu6xYsaLC21aWMWPGoE6dOvj3v/+N06dPm91//fp1vPfee8blhg0bGs9jKvXFF19U+OMEQkNDUatWLSQlJSEpKQlt27Y1OVzm6emJTp064fPPP7cYrh6+tPxharUabdq0MflIgcf517/+BWdnZ3zwwQfGNpVKZfbcXbhwYaU/PiE0NBRqtRoLFy40GffBqwNLRURE4MCBAyafNp6fn48vvvgC/v7+aNq0aaXmYC2hoaFwdHTEp59+arIty5Ytw+3bt8v1Guzbty+uXLli8bOm7t27h/z8fAD3zwl7WOmHHZa+/ks/Y6qscHr8+HEUFhaaXBlG9sE9QFRlhISEYPTo0YiPj8eRI0cQFhYGtVqNM2fOYN26dViwYAFee+01uLq64pNPPsGIESPw4osvYuDAgahZsyaOHj2KgoKCMg9njRgxAn/88Qdefvll1K1bFxcvXsTChQvRunVr4/kOD1Or1Zg3bx4iIyMREhKCAQMGGC+D9/f3x6RJk2xZEqMJEyZg/vz5+OCDD7BmzRq8/fbb2Lx5M1555RUMHz4cgYGByM/Px2+//Yb169fjwoULcHd3R+fOnTFkyBB8+umnOHPmDLp16waDwYDdu3ejc+fOiIqKQlhYmHHP2OjRo3H37l0sXboUnp6eFd7jUpaaNWvi22+/RUREBFq3bm3ySdCHDh3CN998g+DgYGP/ESNGYMyYMfjnP/+Jrl274ujRo9i+fTvc3d0r9LhqtRr/+Mc/sGbNGuTn5+Ojjz4y67No0SJ06NABLVq0wMiRI9GgQQPk5OQgPT0dly9ffuzXGvTq1Qvvvvsu8vLy4Orq+tg51a5dG5GRkVi8eDFOnDiB5557Dq+88gpWr16NGjVqoGnTpkhPT8fOnTsfe+5MWUo/fyo+Ph6vvPIKIiIicPjwYfzwww9mNZw6dSq++eYbdO/eHW+99RZq1aqFVatW4fz589iwYUOFzn2xBQ8PD8TExCAuLg7dunXDq6++ilOnTmHx4sV48cUXy/UBikOGDMHatWsxZswY/PTTT2jfvj30ej1OnjyJtWvXYvv27WjTpg1mz56Nn3/+GT169EC9evVw/fp1LF68GHXr1jWeJN6wYUO4ubkhMTERLi4uqFatGoKCgozBOiUlBc7OzujatatN60LlYIcrz0imSi/RPXjw4CP7lV4eXJYvvvhCBAYGCicnJ+Hi4iJatGghpkyZIq5evWrSb/PmzaJdu3bCyclJuLq6irZt24pvvvnG5HEevAx3/fr1IiwsTHh6egpHR0fxzDPPiNGjR4tr164Z+zx8GXyppKQk8fzzzwuNRiNq1aolBg0aZLys/3HbFRsbK8rzUiy9pPw///mPxfuHDx8uVCqVOHv2rBDi/qXKMTExolGjRsLR0VG4u7uLdu3aiY8++sjkMuWSkhLxn//8RwQEBAhHR0fh4eEhunfvLjIyMkxq2bJlS6HVaoW/v7+YN2+eWL58uQAgzp8/b+xX2cvgS129elVMmjRJNG7cWGi1WuHs7CwCAwPF+++/L27fvm3sp9frxTvvvCPc3d2Fs7OzCA8PF2fPni3zMvhHPedSUlIEAKFQKMSlS5cs9jl37pwYOnSo8Pb2Fmq1Wvj6+opXXnlFrF+//rHblJOTIxwcHMTq1atN2h/1PD937pxQqVTGbfnzzz9FZGSkcHd3F9WrVxfh4eHi5MmT5d5eS89bvV4v4uLiRJ06dYSTk5Po1KmTOHbsmNmYpfN57bXXhJubm9BqtaJt27Ziy5YtFh/jwY9jeNScSp/3j7qk/HF1etBnn30mAgIChFqtFl5eXmLs2LHizz//NOkTEhIimjVrZnH94uJiMW/ePNGsWTOh0WhEzZo1RWBgoIiLizM+91JTU0WvXr2Ej4+PcHR0FD4+PmLAgAFmHzfx3XffiaZNmwoHBwez539QUJAYPHjwY7eHbE8hhMRnoBERycybb76J06dPY/fu3faeCtnRkSNH8MILL+DQoUNm3xNG0mMAIiKysaysLDRu3Bipqakm3whP8lJ61d7atWvtPRUCAxARERHJEK8CIyIiItlhACIiIiLZYQAiIiIi2WEAIiIiItnhByFaYDAYcPXqVbi4uPAL64iIiKoIIQTu3LkDHx+fx35IJwOQBVevXjX75mIiIiKqGi5duoS6des+sg8DkAUuLi4A7hewPB9dXxE6nQ47duwwfo0D2QbrLA3WWRqsszRYZ2nYss55eXnw8/Mzvo8/CgOQBaWHvVxdXW0SgJydneHq6soXmA2xztJgnaXBOkuDdZaGFHUuz+krPAmaiIiIZIcBiIiIiGSHAYiIiIhkhwGIiIiIZIcBiIiIiGSHAYiIiIhkhwGIiIiIZIcBiIiIiGSHAYiIiIhkhwGIiIiIZMeuAejnn39Gz5494ePjA4VCgU2bNj12nbS0NLzwwgvQaDRo1KgRVq5cadZn0aJF8Pf3h1arRVBQEA4cOGD9yRMREVGVZdcAlJ+fj1atWmHRokXl6n/+/Hn06NEDnTt3xpEjRzBx4kSMGDEC27dvN/ZJSkpCdHQ0YmNjcejQIbRq1Qrh4eG4fv26rTaDiIiIqhi7fhlq9+7d0b1793L3T0xMRP369fHxxx8DAJ577jns2bMHn3zyCcLDwwEACQkJGDlyJCIjI43rbN26FcuXL8fUqVOtvxEVkFeowx937uGPIuDKrXtwcNDZdT5/ZyUlJayzBFhnabDO0nha6uzqpIarll/GamtV6tvg09PTERoaatIWHh6OiRMnAgCKi4uRkZGBmJgY4/1KpRKhoaFIT08vc9yioiIUFRUZl/Py8gDc/8Zanc56L4Iv957HRylnADgg7tBuq41LZWGdpcE6S4N1lob966xWKbB+dBCa1nG16zxspfR91Zrvrw+PXR5VKgBlZ2fDy8vLpM3Lywt5eXm4d+8e/vzzT+j1eot9Tp48Wea48fHxiIuLM2vfsWMHnJ2drTN5AGeuKqBW8LxzIiKyrEQAOj2wdsdetPUQ9p6OTaWkpFh9zIKCgnL3rVIByFZiYmIQHR1tXM7Ly4Ofnx/CwsLg6mq9BB4BIF6nQ0pKCrp27Qq1mrs4bUXHOkuCdZYG6yyNp6HOb6zKwO6zN9GqZStEPO9jlznYmi3rXHoEpzyqVADy9vZGTk6OSVtOTg5cXV3h5OQElUoFlUplsY+3t3eZ42o0Gmg0GrN2tVptsxeBLcemv7DO0mCdpcE6S8OedVYo7x8lUKlUf/uftS3qXJHxqtTxmODgYKSmppq0paSkIDg4GADg6OiIwMBAkz4GgwGpqanGPkRERER2DUB3797FkSNHcOTIEQD3L3M/cuQIsrKyANw/NDV06FBj/zFjxiAzMxNTpkzByZMnsXjxYqxduxaTJk0y9omOjsbSpUuxatUqnDhxAmPHjkV+fr7xqjAiIiIiux4C+/XXX9G5c2fjcul5OMOGDcPKlStx7do1YxgCgPr162Pr1q2YNGkSFixYgLp16+K///2v8RJ4AOjXrx9yc3Mxc+ZMZGdno3Xr1khOTjY7MZqIiIjky64BqFOnThCi7LPcLX3Kc6dOnXD48OFHjhsVFYWoqKgnnR4RERH9TVWpc4CIiIiIrIEBiIiIiGSHAYiIiIhkhwGIiIiIZKdKfRAiERER2Y4QAkUlhv9/06O49P86A6ppVKhXu5q9p2g1DEBERERPmRKDAXmFOhTq9CjSGVCo06NQdz+UVPTf0gBTVKL/619jsNGjWF96vwHFesMj55U4+AV0a15HoirYFgMQERHRU+adDb/hnQ2/2XUOCgWgcVBC46DCPd39vUHncvPtOidrYgAiIiJ6SrzwjBt+Pp1rXFYoAK2DChq1EloHFbTq+4Gk9F/NQ8sP/qtVq+DooITW4f7/S/tqHJT329UqY8BxdFD+//8roVGr4KhSQq1SQKFQAACmrD+Ktb9etldZbIIBiIiI6CkxMbQxhgX7Q6lUQKtWwlGlNIYQsi4GICIioqdIzWqO9p6CLPAyeCIiIpIdBiAiIiKSHQYgIiIikh0GICIiIpIdBiAiIiKSHQYgIiIikh0GICIiIpIdBiAiIiKSHQYgIiIieqRqmvufm3w465Z9J2JFDEBERET0SIOCnoFSAew8kYNfL/xh7+lYBQMQERERPVIjTxf0e9EPADB32wkIIew8oyfHAERERESPNTG0MZzUKhzKuoXtv+fYezpPjAGIiIiIHsvLVYsRHesDAD5MPgmd3mDnGT0ZBiAiIiIql1EvNUCtao7IvJGPpIOX7D2dJ8IAREREROXiolVjQpdnAQDzd55BflGJnWdUeQxAREREVG4D2j6DerWdceNuEZbuzrT3dCqNAYiIiIjKzdFBiSnhAQCAL37ORO6dIjvPqHIYgIiIiKhCIlp4o5WfGwqK9fg09UyF138arqJ3sPcEiIiIqGpRKBSI6R6A/l/sx9cHstC3jR+qaVT4s6AYN+8W48+CYvyRr/v//xbjz/xi/FHw///NL4ZOp4JbwE10CvC22zYwABEREVGF/U+D2ugS4InUk9fR87M9FVxbgf2ZfzAAERERUdUTExGAA+f/wJ2iElTXOKBmNTVqVdOglvP9f2s6q1GruiNqOTuiZjVH1K7miNXpF/Dd0Wv2njoDEBEREVVOI08X/DojFEIAWrWqXOtsOXrFxrMqHwYgIiIiqjSNQ/mCz9PG7leBLVq0CP7+/tBqtQgKCsKBAwfK7KvT6TB79mw0bNgQWq0WrVq1QnJyskmfWbNmQaFQmNwCAgJsvRlERERUhdg1ACUlJSE6OhqxsbE4dOgQWrVqhfDwcFy/ft1i/+nTp+Pzzz/HwoULcfz4cYwZMwZ9+vTB4cOHTfo1a9YM165dM9727KnoyVlERET0d2bXAJSQkICRI0ciMjISTZs2RWJiIpydnbF8+XKL/VevXo1p06YhIiICDRo0wNixYxEREYGPP/7YpJ+DgwO8vb2NN3d3dyk2h4iIiKoIu50DVFxcjIyMDMTExBjblEolQkNDkZ6ebnGdoqIiaLVakzYnJyezPTxnzpyBj48PtFotgoODER8fj2eeeabMuRQVFaGo6K9PsszLywNw/5CbTqer8LY9Sul41h6XTLHO0mCdpcE6S4N1lobBcP9b5PV6vc3eY8tDIYR9Po/x6tWr8PX1xb59+xAcHGxsnzJlCnbt2oVffvnFbJ2BAwfi6NGj2LRpExo2bIjU1FT06tULer3eGGB++OEH3L17F02aNMG1a9cQFxeHK1eu4NixY3BxcbE4l1mzZiEuLs6s/euvv4azs7OVtpiIiIg2XlBi1zUlQn0N6PmMwapjFxQUYODAgbh9+zZcXV0f2bdKXQW2YMECjBw5EgEBAVAoFGjYsCEiIyNNDpl1797d+P+WLVsiKCgI9erVw9q1a/Hmm29aHDcmJgbR0dHG5by8PPj5+SEsLOyxBawonU6HlJQUdO3aFWq12qpj019YZ2mwztJgnaXBOksjY8tx4Npl1Pf3R0Q3616kVHoEpzzsFoDc3d2hUqmQk5Nj0p6TkwNvb8ufDOnh4YFNmzahsLAQN2/ehI+PD6ZOnYoGDRqU+Thubm5o3Lgxzp49W2YfjUYDjUZj1q5Wq232IrDl2PQX1lkarLM0WGdpsM62pVTeP/1YpVJZvc4VGc9uJ0E7OjoiMDAQqampxjaDwYDU1FSTQ2KWaLVa+Pr6oqSkBBs2bECvXr3K7Hv37l2cO3cOderUsdrciYiIqGqz61Vg0dHRWLp0KVatWoUTJ05g7NixyM/PR2RkJABg6NChJidJ//LLL9i4cSMyMzOxe/dudOvWDQaDAVOmTDH2mTx5Mnbt2oULFy5g37596NOnD1QqFQYMGCD59hEREdHTya7nAPXr1w+5ubmYOXMmsrOz0bp1ayQnJ8PLywsAkJWVZdxVBgCFhYWYPn06MjMzUb16dURERGD16tVwc3Mz9rl8+TIGDBiAmzdvwsPDAx06dMD+/fvh4eEh9eYRERHRU8ruJ0FHRUUhKirK4n1paWkmyyEhITh+/Pgjx1uzZo21pkZERER/U3b/KgwiIiIiqTEAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7Ng9AC1atAj+/v7QarUICgrCgQMHyuyr0+kwe/ZsNGzYEFqtFq1atUJycvITjUlERETyY9cAlJSUhOjoaMTGxuLQoUNo1aoVwsPDcf36dYv9p0+fjs8//xwLFy7E8ePHMWbMGPTp0weHDx+u9JhEREQkP3YNQAkJCRg5ciQiIyPRtGlTJCYmwtnZGcuXL7fYf/Xq1Zg2bRoiIiLQoEEDjB07FhEREfj4448rPSYRERHJj4O9Hri4uBgZGRmIiYkxtimVSoSGhiI9Pd3iOkVFRdBqtSZtTk5O2LNnT6XHLB23qKjIuJyXlwfg/iE3nU5X8Y17hNLxrD0umWKdpcE6S4N1lgbrLA2DwQAA0Ov1NnuPLQ+7BaAbN25Ar9fDy8vLpN3LywsnT560uE54eDgSEhLw0ksvoWHDhkhNTcXGjRuh1+srPSYAxMfHIy4uzqx9x44dcHZ2ruimlUtKSopNxiVTrLM0WGdpsM7SYJ1t62KWEoAS5y9cwLZtmVYdu6CgoNx97RaAKmPBggUYOXIkAgICoFAo0LBhQ0RGRj7x4a2YmBhER0cbl/Py8uDn54ewsDC4uro+6bRN6HQ6pKSkoGvXrlCr1VYdm/7COkuDdZYG6ywN1lkaGVuOA9cuo76/PyK6BVh17NIjOOVhtwDk7u4OlUqFnJwck/acnBx4e3tbXMfDwwObNm1CYWEhbt68CR8fH0ydOhUNGjSo9JgAoNFooNFozNrVarXNXgS2HJv+wjpLg3WWBussDdbZtpTK+6cfq1Qqq9e5IuPZ7SRoR0dHBAYGIjU11dhmMBiQmpqK4ODgR66r1Wrh6+uLkpISbNiwAb169XriMYmIiEg+7HoILDo6GsOGDUObNm3Qtm1bzJ8/H/n5+YiMjAQADB06FL6+voiPjwcA/PLLL7hy5Qpat26NK1euYNasWTAYDJgyZUq5xyQiIiKyawDq168fcnNzMXPmTGRnZ6N169ZITk42nsSclZVl3FUGAIWFhZg+fToyMzNRvXp1REREYPXq1XBzcyv3mERERER2Pwk6KioKUVFRFu9LS0szWQ4JCcHx48efaEwiIiIiu38VBhEREZHUGICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHbsHoAWLVoEf39/aLVaBAUF4cCBA4/sP3/+fDRp0gROTk7w8/PDpEmTUFhYaLx/1qxZUCgUJreAgABbbwYRERFVIQ72fPCkpCRER0cjMTERQUFBmD9/PsLDw3Hq1Cl4enqa9f/6668xdepULF++HO3atcPp06cxfPhwKBQKJCQkGPs1a9YMO3fuNC47ONh1M4mIiOgpY9c9QAkJCRg5ciQiIyPRtGlTJCYmwtnZGcuXL7fYf9++fWjfvj0GDhwIf39/hIWFYcCAAWZ7jRwcHODt7W28ubu7S7E5REREVEXYLQAVFxcjIyMDoaGhf01GqURoaCjS09MtrtOuXTtkZGQYA09mZia2bduGiIgIk35nzpyBj48PGjRogEGDBiErK8t2G0JERERVjt2ODd24cQN6vR5eXl4m7V5eXjh58qTFdQYOHIgbN26gQ4cOEEKgpKQEY8aMwbRp04x9goKCsHLlSjRp0gTXrl1DXFwcOnbsiGPHjsHFxcXiuEVFRSgqKjIu5+XlAQB0Oh10Ot2TbqqJ0vGsPS6ZYp2lwTpLg3WWBussDYPBAADQ6/U2e48tjyp1ckxaWhrmzp2LxYsXIygoCGfPnsWECRMwZ84czJgxAwDQvXt3Y/+WLVsiKCgI9erVw9q1a/Hmm29aHDc+Ph5xcXFm7Tt27ICzs7NNtiUlJcUm45Ip1lkarLM0WGdpsM62dTFLCUCJ8xcuYNu2TKuOXVBQUO6+dgtA7u7uUKlUyMnJMWnPycmBt7e3xXVmzJiBIUOGYMSIEQCAFi1aID8/H6NGjcK7774LpdL8iJ6bmxsaN26Ms2fPljmXmJgYREdHG5fz8vLg5+eHsLAwuLq6VmbzyqTT6ZCSkoKuXbtCrVZbdWz6C+ssDdZZGqyzNFhnaWRsOQ5cu4z6/v6I6Gbdq7RLj+CUh90CkKOjIwIDA5GamorevXsDuL9bLDU1FVFRURbXKSgoMAs5KpUKACCEsLjO3bt3ce7cOQwZMqTMuWg0Gmg0GrN2tVptsxeBLcemv7DO0mCdpcE6S4N1tq3S93GVSmX1OldkPLseAouOjsawYcPQpk0btG3bFvPnz0d+fj4iIyMBAEOHDoWvry/i4+MBAD179kRCQgKef/554yGwGTNmoGfPnsYgNHnyZPTs2RP16tXD1atXERsbC5VKhQEDBthtO4mIiOjpYtcA1K9fP+Tm5mLmzJnIzs5G69atkZycbDwxOisry2SPz/Tp06FQKDB9+nRcuXIFHh4e6NmzJ95//31jn8uXL2PAgAG4efMmPDw80KFDB+zfvx8eHh6Sbx8RERE9nex+EnRUVFSZh7zS0tJMlh0cHBAbG4vY2Ngyx1uzZo01p0dERER/Q3b/KgwiIiIiqVVqD5Ber8fKlSuRmpqK69evG6/pL/Xjjz9aZXJEREREtlCpADRhwgSsXLkSPXr0QPPmzaFQKKw9LyIiIiKbqVQAWrNmDdauXWv2FRREREREVUGlzgFydHREo0aNrD0XIiIiIklUKgD9+9//xoIFC8r88EEiIiKip1mlDoHt2bMHP/30E3744Qc0a9bM7JMXN27caJXJEREREdlCpQKQm5sb+vTpY+25EBEREUmiUgFoxYoV1p4HERERkWSe6JOgc3NzcerUKQBAkyZN+HUTREREVCVU6iTo/Px8vPHGG6hTpw5eeuklvPTSS/Dx8cGbb76JgoICa8+RiIiIyKoqFYCio6Oxa9cufP/997h16xZu3bqF7777Drt27cK///1va8+RiIiIyKoqdQhsw4YNWL9+PTp16mRsi4iIgJOTE/r27YslS5ZYa35EREREVlepPUAFBQXw8vIya/f09OQhMCIiInrqVSoABQcHIzY2FoWFhca2e/fuIS4uDsHBwVabHBEREZEtVOoQ2IIFCxAeHo66deuiVatWAICjR49Cq9Vi+/btVp0gERERkbVVKgA1b94cZ86cwVdffYWTJ08CAAYMGIBBgwbBycnJqhMkIiIisrZKfw6Qs7MzRo4cac25EBEREUmi3AFo8+bN6N69O9RqNTZv3vzIvq+++uoTT4yIiIjIVsodgHr37o3s7Gx4enqid+/eZfZTKBTQ6/XWmBsRERGRTZQ7ABkMBov/JyIiIqpqKnUZvCW3bt2y1lBERERENlWpADRv3jwkJSUZl19//XXUqlULvr6+OHr0qNUmR0RERGQLlQpAiYmJ8PPzAwCkpKRg586dSE5ORvfu3fH2229bdYJERERE1lapy+Czs7ONAWjLli3o27cvwsLC4O/vj6CgIKtOkIiIiMjaKrUHqGbNmrh06RIAIDk5GaGhoQAAIQSvACMiIqKnXqX2AP3jH//AwIED8eyzz+LmzZvo3r07AODw4cNo1KiRVSdIREREZG2VCkCffPIJ/P39cenSJXz44YeoXr06AODatWsYN26cVSdIREREZG2VCkBqtRqTJ082a580adITT4iIiIjI1vhVGERERCQ7/CoMIiIikh1+FQYRERHJjtW+CoOIiIioqqhUAHrrrbfw6aefmrV/9tlnmDhx4pPOiYiIiMimKhWANmzYgPbt25u1t2vXDuvXr6/QWIsWLYK/vz+0Wi2CgoJw4MCBR/afP38+mjRpAicnJ/j5+WHSpEkoLCx8ojGJiIhIXioVgG7evIkaNWqYtbu6uuLGjRvlHicpKQnR0dGIjY3FoUOH0KpVK4SHh+P69esW+3/99deYOnUqYmNjceLECSxbtgxJSUmYNm1apcckIiIi+alUAGrUqBGSk5PN2n/44Qc0aNCg3OMkJCRg5MiRiIyMRNOmTZGYmAhnZ2csX77cYv99+/ahffv2GDhwIPz9/REWFoYBAwaY7OGp6JhEREQkP5X6IMTo6GhERUUhNzcXL7/8MgAgNTUVH3/8MebPn1+uMYqLi5GRkYGYmBhjm1KpRGhoKNLT0y2u065dO/zv//4vDhw4gLZt2yIzMxPbtm3DkCFDKj0mABQVFaGoqMi4nJeXBwDQ6XTQ6XTl2p7yKh3P2uOSKdZZGqyzNFhnabDO0ii9klyv19vsPbY8KhWA3njjDRQVFeH999/HnDlzAAD+/v5YsmQJhg4dWq4xbty4Ab1eDy8vL5N2Ly8vnDx50uI6AwcOxI0bN9ChQwcIIVBSUoIxY8YYD4FVZkwAiI+PR1xcnFn7jh074OzsXK7tqaiUlBSbjEumWGdpsM7SYJ2lwTrb1sUsJQAlzl+4gG3bMq06dkFBQbn7VioAAcDYsWMxduxY5ObmwsnJyfh9YLaUlpaGuXPnYvHixQgKCsLZs2cxYcIEzJkzBzNmzKj0uDExMYiOjjYu5+Xlwc/PD2FhYXB1dbXG1I10Oh1SUlLQtWtXqNVqq45Nf2GdpcE6S4N1lgbrLI2MLceBa5dR398fEd0CrDp26RGc8qh0ACopKUFaWhrOnTuHgQMHAgCuXr0KV1fXcoUhd3d3qFQq5OTkmLTn5OTA29vb4jozZszAkCFDMGLECABAixYtkJ+fj1GjRuHdd9+t1JgAoNFooNFozNrVarXNXgS2HJv+wjpLg3WWBussDdbZtpTK+6cfq1Qqq9e5IuNV6iToixcvokWLFujVqxfGjx+P3NxcAMC8efMsfkmqJY6OjggMDERqaqqxzWAwIDU1FcHBwRbXKSgoMBaulEqlAgAIISo1JhEREclPpQLQhAkT0KZNG/z5559wcnIytvfp08ckfDxOdHQ0li5dilWrVuHEiRMYO3Ys8vPzERkZCQAYOnSoyQnNPXv2xJIlS7BmzRqcP38eKSkpmDFjBnr27GkMQo8bk4iIiKhSh8B2796Nffv2wdHR0aTd398fV65cKfc4/fr1Q25uLmbOnIns7Gy0bt0aycnJxpOYs7KyTPb4TJ8+HQqFAtOnT8eVK1fg4eGBnj174v333y/3mERERESVCkAGg8HiN75fvnwZLi4uFRorKioKUVFRFu9LS0szWXZwcEBsbCxiY2MrPSYRERFRpQ6BhYWFmXzej0KhwN27dxEbG4uIiAhrzY2IiIjIJiq1B+ijjz5Ct27d0LRpUxQWFmLgwIE4c+YM3N3d8c0331h7jkRERERWVakA5Ofnh6NHjyIpKQlHjx7F3bt38eabb2LQoEEmJ0UTERERPY0qHIB0Oh0CAgKwZcsWDBo0CIMGDbLFvIiIiIhspsLnAKnVahQWFtpiLkRERESSqNRJ0OPHj8e8efNQUlJi7fkQERER2VylzgE6ePAgUlNTsWPHDrRo0QLVqlUzuX/jxo1WmRwRERGRLVQqALm5ueGf//yntedCREREJIkKBSCDwYD//Oc/OH36NIqLi/Hyyy9j1qxZvPKLiIiIqpQKnQP0/vvvY9q0aahevTp8fX3x6aefYvz48baaGxEREZFNVCgAffnll1i8eDG2b9+OTZs24fvvv8dXX30Fg8Fgq/kRERERWV2FAlBWVpbJV12EhoZCoVDg6tWrVp8YERERka1UKACVlJRAq9WatKnVauh0OqtOioiIiMiWKnQStBACw4cPh0ajMbYVFhZizJgxJpfC8zJ4IiIieppVKAANGzbMrG3w4MFWmwwRERGRFCoUgFasWGGreRARERFJplJfhUFERERUlTEAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkewwABEREZHsMAARERGR7DAAERERkew8FQFo0aJF8Pf3h1arRVBQEA4cOFBm306dOkGhUJjdevToYewzfPhws/u7desmxaYQERFRFeBg7wkkJSUhOjoaiYmJCAoKwvz58xEeHo5Tp07B09PTrP/GjRtRXFxsXL558yZatWqF119/3aRft27dsGLFCuOyRqOx3UYQERFRlWL3PUAJCQkYOXIkIiMj0bRpUyQmJsLZ2RnLly+32L9WrVrw9vY23lJSUuDs7GwWgDQajUm/mjVrSrE5REREVAXYNQAVFxcjIyMDoaGhxjalUonQ0FCkp6eXa4xly5ahf//+qFatmkl7WloaPD090aRJE4wdOxY3b9606tyJiIio6rLrIbAbN25Ar9fDy8vLpN3LywsnT5587PoHDhzAsWPHsGzZMpP2bt264R//+Afq16+Pc+fOYdq0aejevTvS09OhUqnMxikqKkJRUZFxOS8vDwCg0+mg0+kqs2llKh3P2uOSKdZZGqyzNFhnabDO0jAYDAAAvV5vs/fY8rD7OUBPYtmyZWjRogXatm1r0t6/f3/j/1u0aIGWLVuiYcOGSEtLQ5cuXczGiY+PR1xcnFn7jh074OzsbP2JA0hJSbHJuGSKdZYG6ywN1lkarLNtXcxSAlDi/IUL2LYt06pjFxQUlLuvXQOQu7s7VCoVcnJyTNpzcnLg7e39yHXz8/OxZs0azJ49+7GP06BBA7i7u+Ps2bMWA1BMTAyio6ONy3l5efDz80NYWBhcXV3LuTXlo9PpkJKSgq5du0KtVlt1bPoL6ywN1lkarLM0WGdpZGw5Dly7jPr+/ojoFmDVsUuP4JSHXQOQo6MjAgMDkZqait69ewO4v2ssNTUVUVFRj1x33bp1KCoqwuDBgx/7OJcvX8bNmzdRp04di/drNBqLV4mp1WqbvQhsOTb9hXWWBussDdZZGqyzbSmV908/VqlUVq9zRcaz+1Vg0dHRWLp0KVatWoUTJ05g7NixyM/PR2RkJABg6NChiImJMVtv2bJl6N27N2rXrm3SfvfuXbz99tvYv38/Lly4gNTUVPTq1QuNGjVCeHi4JNtERERETze7nwPUr18/5ObmYubMmcjOzkbr1q2RnJxsPDE6KyvLmBZLnTp1Cnv27MGOHTvMxlOpVPi///s/rFq1Crdu3YKPjw/CwsIwZ84cfhYQERERAXgKAhAAREVFlXnIKy0tzaytSZMmEEJY7O/k5ITt27dbc3pERET0N2P3Q2BEREREUmMAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2XkqAtCiRYvg7+8PrVaLoKAgHDhwoMy+nTp1gkKhMLv16NHD2EcIgZkzZ6JOnTpwcnJCaGgozpw5I8WmEBERURVg9wCUlJSE6OhoxMbG4tChQ2jVqhXCw8Nx/fp1i/03btyIa9euGW/Hjh2DSqXC66+/buzz4Ycf4tNPP0ViYiJ++eUXVKtWDeHh4SgsLJRqs4iIiOgpZvcAlJCQgJEjRyIyMhJNmzZFYmIinJ2dsXz5cov9a9WqBW9vb+MtJSUFzs7OxgAkhMD8+fMxffp09OrVCy1btsSXX36Jq1evYtOmTRJuGRERET2tHOz54MXFxcjIyEBMTIyxTalUIjQ0FOnp6eUaY9myZejfvz+qVasGADh//jyys7MRGhpq7FOjRg0EBQUhPT0d/fv3NxujqKgIRUVFxuW8vDwAgE6ng06nq9S2laV0PGuPS6ZYZ2mwztJgnaXBOkvDYDAAAPR6vc3eY8vDrgHoxo0b0Ov18PLyMmn38vLCyZMnH7v+gQMHcOzYMSxbtszYlp2dbRzj4TFL73tYfHw84uLizNp37NgBZ2fnx86jMlJSUmwyLplinaXBOkuDdZYG62xbF7OUAJQ4f+ECtm3LtOrYBQUF5e5r1wD0pJYtW4YWLVqgbdu2TzROTEwMoqOjjct5eXnw8/NDWFgYXF1dn3SaJnQ6HVJSUtC1a1eo1Wqrjk1/YZ2lwTpLg3WWBussjYwtx4Frl1Hf3x8R3QKsOnbpEZzysGsAcnd3h0qlQk5Ojkl7Tk4OvL29H7lufn4+1qxZg9mzZ5u0l66Xk5ODOnXqmIzZunVri2NpNBpoNBqzdrVabbMXgS3Hpr+wztJgnaXBOkuDdbYtpfL+6ccqlcrqda7IeHY9CdrR0RGBgYFITU01thkMBqSmpiI4OPiR665btw5FRUUYPHiwSXv9+vXh7e1tMmZeXh5++eWXx45JRERE8mD3Q2DR0dEYNmwY2rRpg7Zt22L+/PnIz89HZGQkAGDo0KHw9fVFfHy8yXrLli1D7969Ubt2bZN2hUKBiRMn4r333sOzzz6L+vXrY8aMGfDx8UHv3r2l2iwiIiJ6itk9APXr1w+5ubmYOXMmsrOz0bp1ayQnJxtPYs7KyjLuLit16tQp7NmzBzt27LA45pQpU5Cfn49Ro0bh1q1b6NChA5KTk6HVam2+PURERPT0s3sAAoCoqChERUVZvC8tLc2srUmTJhBClDmeQqHA7Nmzzc4PIiIiIgKegg9CJCIiIpIaAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyQ4DEBEREckOAxARERHJDgMQERERyY7dA9CiRYvg7+8PrVaLoKAgHDhw4JH9b926hfHjx6NOnTrQaDRo3Lgxtm3bZrx/1qxZUCgUJreAgABbbwYRERFVIQ72fPCkpCRER0cjMTERQUFBmD9/PsLDw3Hq1Cl4enqa9S8uLkbXrl3h6emJ9evXw9fXFxcvXoSbm5tJv2bNmmHnzp3GZQcHu24mERERPWXsmgwSEhIwcuRIREZGAgASExOxdetWLF++HFOnTjXrv3z5cvzxxx/Yt28f1Go1AMDf39+sn4ODA7y9vW06dyIiIqq67HYIrLi4GBkZGQgNDf1rMkolQkNDkZ6ebnGdzZs3Izg4GOPHj4eXlxeaN2+OuXPnQq/Xm/Q7c+YMfHx80KBBAwwaNAhZWVk23RYiIiKqWuy2B+jGjRvQ6/Xw8vIyaffy8sLJkyctrpOZmYkff/wRgwYNwrZt23D27FmMGzcOOp0OsbGxAICgoCCsXLkSTZo0wbVr1xAXF4eOHTvi2LFjcHFxsThuUVERioqKjMt5eXkAAJ1OB51OZ43NNSodz9rjkinWWRqsszRYZ2mwztIwGAwAAL1eb7P32PJQCCGEVR+9nK5evQpfX1/s27cPwcHBxvYpU6Zg165d+OWXX8zWady4MQoLC3H+/HmoVCoA9w+j/ec//8G1a9csPs6tW7dQr149JCQk4M0337TYZ9asWYiLizNr//rrr+Hs7FyZzSMiIiILNl5QYtc1JUJ9Dej5jMGqYxcUFGDgwIG4ffs2XF1dH9nXbnuA3N3doVKpkJOTY9Kek5NT5vk7derUgVqtNoYfAHjuueeQnZ2N4uJiODo6mq3j5uaGxo0b4+zZs2XOJSYmBtHR0cblvLw8+Pn5ISws7LEFrCidToeUlBR07drVeB4TWR/rLA3WWRqsszRYZ2lkbDkOXLuM+v7+iOhm3au0S4/glIfdApCjoyMCAwORmpqK3r17A7i/Wyw1NRVRUVEW12nfvj2+/vprGAwGKJX3T186ffo06tSpYzH8AMDdu3dx7tw5DBkypMy5aDQaaDQas3a1Wm2zF4Etx6a/sM7SYJ2lwTpLg3W2rdL3b5VKZfU6V2Q8u34OUHR0NJYuXYpVq1bhxIkTGDt2LPLz841XhQ0dOhQxMTHG/mPHjsUff/yBCRMm4PTp09i6dSvmzp2L8ePHG/tMnjwZu3btwoULF7Bv3z706dMHKpUKAwYMkHz7iIiI6Olk18vg+/Xrh9zcXMycORPZ2dlo3bo1kpOTjSdGZ2VlGZMiAPj5+WH79u2YNGkSWrZsCV9fX0yYMAHvvPOOsc/ly5cxYMAA3Lx5Ex4eHujQoQP2798PDw8PybePiIiInk52/4TAqKioMg95paWlmbUFBwdj//79ZY63Zs0aa02NiIiI/qbs/lUYRERERFJjACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiIsk4qJRQKwRUSoV952HXRyciIiJZeSe8MVrozyKiSyO7zoN7gIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhiAiIiISHYYgIiIiEh2GICIiIhIdhzsPYGnkRACAJCXl2f1sXU6HQoKCpCXlwe1Wm318ek+1lkarLM0WGdpsM7SsGWdS9+3S9/HH4UByII7d+4AAPz8/Ow8EyIiIqqoO3fuoEaNGo/soxDliUkyYzAYcPXqVbi4uEChUFh17Ly8PPj5+eHSpUtwdXW16tj0F9ZZGqyzNFhnabDO0rBlnYUQuHPnDnx8fKBUPvosH+4BskCpVKJu3bo2fQxXV1e+wCTAOkuDdZYG6ywN1lkatqrz4/b8lOJJ0ERERCQ7DEBEREQkOwxAEtNoNIiNjYVGo7H3VP7WWGdpsM7SYJ2lwTpL42mpM0+CJiIiItnhHiAiIiKSHQYgIiIikh0GICIiIpIdBiAiIiKSHQYgG1i0aBH8/f2h1WoRFBSEAwcOPLL/unXrEBAQAK1WixYtWmDbtm0SzbRqq0idly5dio4dO6JmzZqoWbMmQkNDH/tzofsq+nwutWbNGigUCvTu3du2E/ybqGidb926hfHjx6NOnTrQaDRo3Lgxf3eUQ0XrPH/+fDRp0gROTk7w8/PDpEmTUFhYKNFsq6aff/4ZPXv2hI+PDxQKBTZt2vTYddLS0vDCCy9Ao9GgUaNGWLlypc3nCUFWtWbNGuHo6CiWL18ufv/9dzFy5Ejh5uYmcnJyLPbfu3evUKlU4sMPPxTHjx8X06dPF2q1Wvz2228Sz7xqqWidBw4cKBYtWiQOHz4sTpw4IYYPHy5q1KghLl++LPHMq5aK1rnU+fPnha+vr+jYsaPo1auXNJOtwipa56KiItGmTRsREREh9uzZI86fPy/S0tLEkSNHJJ551VLROn/11VdCo9GIr776Spw/f15s375d1KlTR0yaNEnimVct27ZtE++++67YuHGjACC+/fbbR/bPzMwUzs7OIjo6Whw/flwsXLhQqFQqkZycbNN5MgBZWdu2bcX48eONy3q9Xvj4+Ij4+HiL/fv27St69Ohh0hYUFCRGjx5t03lWdRWt88NKSkqEi4uLWLVqla2m+LdQmTqXlJSIdu3aif/+979i2LBhDEDlUNE6L1myRDRo0EAUFxdLNcW/hYrWefz48eLll182aYuOjhbt27e36Tz/TsoTgKZMmSKaNWtm0tavXz8RHh5uw5kJwUNgVlRcXIyMjAyEhoYa25RKJUJDQ5Genm5xnfT0dJP+ABAeHl5mf6pcnR9WUFAAnU6HWrVq2WqaVV5l6zx79mx4enrizTfflGKaVV5l6rx582YEBwdj/Pjx8PLyQvPmzTF37lzo9Xqppl3lVKbO7dq1Q0ZGhvEwWWZmJrZt24aIiAhJ5iwX9nof5JehWtGNGzeg1+vh5eVl0u7l5YWTJ09aXCc7O9ti/+zsbJvNs6qrTJ0f9s4778DHx8fsRUd/qUyd9+zZg2XLluHIkSMSzPDvoTJ1zszMxI8//ohBgwZh27ZtOHv2LMaNGwedTofY2Fgppl3lVKbOAwcOxI0bN9ChQwcIIVBSUoIxY8Zg2rRpUkxZNsp6H8zLy8O9e/fg5ORkk8flHiCSnQ8++ABr1qzBt99+C61Wa+/p/G3cuXMHQ4YMwdKlS+Hu7m7v6fytGQwGeHp64osvvkBgYCD69euHd999F4mJifae2t9KWloa5s6di8WLF+PQoUPYuHEjtm7dijlz5th7amQF3ANkRe7u7lCpVMjJyTFpz8nJgbe3t8V1vL29K9SfKlfnUh999BE++OAD7Ny5Ey1btrTlNKu8itb53LlzuHDhAnr27GlsMxgMAAAHBwecOnUKDRs2tO2kq6DKPJ/r1KkDtVoNlUplbHvuueeQnZ2N4uJiODo62nTOVVFl6jxjxgwMGTIEI0aMAAC0aNEC+fn5GDVqFN59910oldyHYA1lvQ+6urrabO8PwD1AVuXo6IjAwECkpqYa2wwGA1JTUxEcHGxxneDgYJP+AJCSklJmf6pcnQHgww8/xJw5c5CcnIw2bdpIMdUqraJ1DggIwG+//YYjR44Yb6+++io6d+6MI0eOwM/PT8rpVxmVeT63b98eZ8+eNQZMADh9+jTq1KnD8FOGytS5oKDALOSUhk7Br9G0Gru9D9r0FGsZWrNmjdBoNGLlypXi+PHjYtSoUcLNzU1kZ2cLIYQYMmSImDp1qrH/3r17hYODg/joo4/EiRMnRGxsLC+DL4eK1vmDDz4Qjo6OYv369eLatWvG2507d+y1CVVCRev8MF4FVj4VrXNWVpZwcXERUVFR4tSpU2LLli3C09NTvPfee/bahCqhonWOjY0VLi4u4ptvvhGZmZlix44domHDhqJv37722oQq4c6dO+Lw4cPi8OHDAoBISEgQhw8fFhcvXhRCCDF16lQxZMgQY//Sy+DffvttceLECbFo0SJeBl9VLVy4UDzzzDPC0dFRtG3bVuzfv994X0hIiBg2bJhJ/7Vr14rGjRsLR0dH0axZM7F161aJZ1w1VaTO9erVEwDMbrGxsdJPvIqp6PP5QQxA5VfROu/bt08EBQUJjUYjGjRoIN5//31RUlIi8ayrnorUWafTiVmzZomGDRsKrVYr/Pz8xLhx48Sff/4p/cSrkJ9++sni79vS2g4bNkyEhISYrdO6dWvh6OgoGjRoIFasWGHzeSqE4H48IiIikheeA0RERESywwBEREREssMARERERLLDAERERESywwBEREREssMARERERLLDAERERESywwBERFROCoUCmzZtAgBcuHABCoUCR44cseuciKhyGICIqEoYPnw4FAoFFAoF1Go16tevjylTpqCwsNDeUyOiKojfBk9EVUa3bt2wYsUK6HQ6ZGRkYNiwYVAoFJg3b569p0ZEVQz3ABFRlaHRaODt7Q0/Pz/07t0boaGhSElJAXD/m73j4+NRv359ODk5oVWrVli/fr3J+r///jteeeUVuLq6wsXFBR07dsS5c+cAAAcPHkTXrl3h7u6OGjVqICQkBIcOHZJ8G4lIGgxARFQlHTt2DPv27YOjoyMAID4+Hl9++SUSExPx+++/Y9KkSRg8eDB27doFALhy5QpeeuklaDQa/Pjjj8jIyMAbb7yBkpISAMCdO3cwbNgw7NmzB/v378ezzz6LiIgI3Llzx27bSES2w0NgRFRlbNmyBdWrV0dJSQmKioqgVCrx2WefoaioCHPnzsXOnTsRHBwMAGjQoAH27NmDzz//HCEhIVi0aBFq1KiBNWvWQK1WAwAaN25sHPvll182eawvvvgCbm5u2LVrF1555RXpNpKIJMEARERVRufOnbFkyRLk5+fjk08+gYODA/75z3/i999/R0FBAbp27WrSv7i4GM8//zwA4MiRI+jYsaMx/DwsJycH06dPR1paGq5fvw69Xo+CggJkZWXZfLuISHoMQERUZVSrVg2NGjUCACxfvhytWrXCsmXL0Lx5cwDA1q1b4evra7KORqMBADg5OT1y7GHDhuHmzZtYsGAB6tWrB41Gg+DgYBQXF9tgS4jI3hiAiKhKUiqVmDZtGqKjo3H69GloNBpkZWUhJCTEYv+WLVti1apV0Ol0FvcC7d27F4sXL0ZERAQA4NKlS7hx44ZNt4GI7IcnQRNRlfX6669DpVLh888/x+TJkzFp0iSsWrUK586dw6FDh7Bw4UKsWrUKABAVFYW8vDz0798fv/76K86cOYPVq1fj1KlTAIBnn30Wq1evxokTJ/DLL79g0KBBj91rRERVF/cAEVGV5eDggKioKHz44Yc4f/48PDw8EB8fj8zMTLi5ueGFF17AtGnTAAC1a9fGjz/+iLfffhshISFQqVRo3bo12rdvDwBYtmwZRo0ahRdeeAF+fn6YO3cuJk+ebM/NIyIbUgghhL0nQURERCQlHgIjIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZYQAiIiIi2WEAIiIiItlhACIiIiLZ+X8H/tKWFKf8nAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "uAcIaAnMGddt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Stacking model\n",
        "stack = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=500)\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "pred = stack.predict(X_test)\n",
        "\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-afBh7dtD17u",
        "outputId": "61475584-8508-41d7-c21b-38cbd444a904"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "cmiUYLNgGfrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "sample_sizes = [0.3, 0.5, 0.7, 1.0]   # fractions of dataset\n",
        "\n",
        "print(\"Bagging Regressor with Different Bootstrap Sample Sizes:\\n\")\n",
        "\n",
        "for size in sample_sizes:\n",
        "    bag = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        max_samples=size,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    pred = bag.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    print(f\"max_samples={size} → MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILQk6ppbG4JD",
        "outputId": "95eeb142-ffc7-4a77-a833-4a894e188206"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with Different Bootstrap Sample Sizes:\n",
            "\n",
            "max_samples=0.3 → MSE: 0.2797\n",
            "max_samples=0.5 → MSE: 0.2664\n",
            "max_samples=0.7 → MSE: 0.2638\n",
            "max_samples=1.0 → MSE: 0.2573\n"
          ]
        }
      ]
    }
  ]
}